{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem Set 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzgz3HnDfvsZ",
        "colab_type": "text"
      },
      "source": [
        "*CSE 555 - Intro to Pattern Recognition**\n",
        "\n",
        "# **Problem Set 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQY60VEtgHRd",
        "colab_type": "text"
      },
      "source": [
        "## **Problem Statement**\n",
        "\n",
        "In this problem, we will train a neural network to identify the digit on a image in the MNIST dataset using Tensorflow (or) Keras. This neural network (NN) has 10 softmax outpput notes generating $log(t=m|x;w)$ where $m=0,1,...,9$. Let $x_n \\in R^{28\\times28}$ be the $28\\times28$, $t_n$ be the label of the image $x_n$, $w$ be the synaptic weights of the NN, an $n$ be the index of a pattern in the training dataset.\n",
        "\n",
        "2. \n",
        "- Build a NN with 1 hidden layer of 30 sigmoid nodes, and an output layer 10 softmax nodes from 1000 training images (100 images per digit). Train the network for 30 complete wpochs, using mini-batches of 10 training examples at a time, a learning rate $\\eta = 0.1$. Plot the training error, testing error, criterion function on training dataset, criterion function on testing dataset of a separate 1000 testing images (100 images per digit), and the learning speed of the hidden layer (the average absolute changes of weights divided by the values of the weights).\n",
        "\n",
        "- Repeat 2(a) with 2 hidden layers of 30 sigmoid nodes each, 3 hidden layers of 30 sigmoid nodes each, and with & without L2 regularization $\\lambda|w|^2$ and $\\lambda=5$. (You will repeat w(a) for 5 times: 1 for 2 hidden layer network; 1 for 3 hidden layer network; and once each for 1, 2, 3 hidden layers with regularization).\n",
        "\n",
        "- Construct and train convolutional NN (CNN) for MNIST classification. Regularize the training of the NN through dropout. Regularize the training of NN through augment your selection of 1000 images by rotating them for 1-3 degrees clockwise and counter clockwise, and shifting them for 3 pixels in 8 different directions. You can find many tutorials on those techniques, and our emphasize is that we understand those techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--vHjFFhF0Ns",
        "colab_type": "text"
      },
      "source": [
        "### **2 a & b**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a1F3Aj5zZzh",
        "colab_type": "text"
      },
      "source": [
        "Importing requried libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE0xR5wQf6y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGHC7_UizfUi",
        "colab_type": "text"
      },
      "source": [
        "Loading MNIST training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG-yiV90fr6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "mnist_data = keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist_data.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx2boGKYzit2",
        "colab_type": "text"
      },
      "source": [
        "Splitting training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxPujBWAhwHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "validation_x = x_train[-10000:]\n",
        "validation_y = y_train[-10000:]\n",
        "\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yRJav9Mz2Ub",
        "colab_type": "text"
      },
      "source": [
        "Seggregating 100 images per category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEKEi2e1hzFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "categories = range(10)\n",
        "categorical_data = {i:[] for i in categories}\n",
        "\n",
        "for i in range(x_train.shape[0]):\n",
        "  categorical_data[y_train[i]].append(x_train[i])\n",
        "\n",
        "temp_x = []\n",
        "temp_y = []\n",
        "\n",
        "for i in categories:\n",
        "  for j in range(100):\n",
        "    temp_x.append(categorical_data[i][j])\n",
        "    temp_y.append(i)\n",
        "\n",
        "x_train = np.asarray(temp_x)\n",
        "y_train = np.asarray(temp_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhNFwHB7zLiU",
        "colab_type": "text"
      },
      "source": [
        "Initializing class for properties of required neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBl2hBVEjXKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class model_specs:\n",
        "  batch = 10\n",
        "  eta = 0.1\n",
        "  epoch = 30\n",
        "  no_sigmoids = 30\n",
        "  no_softmaxs = 10\n",
        "  reg_lambda = 5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_XeW3PkzHtp",
        "colab_type": "text"
      },
      "source": [
        "Initializing class for neural network (NN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nWiMA37jPKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class nn:\n",
        "  def __init__(self, reg, layer):\n",
        "    self.model = keras.models.Sequential()\n",
        "    self.model.add(keras.layers.Flatten())\n",
        "\n",
        "    for each_layer in range(layer):\n",
        "      if(reg):\n",
        "        self.model.add(keras.layers.Dense(units = model_specs.no_sigmoids, activation = \"sigmoid\", kernel_regularizer = keras.regularizers.l2(model_specs.reg_lambda)))\n",
        "      else:\n",
        "        self.model.add(keras.layers.Dense(units = model_specs.no_sigmoids, activation = \"sigmoid\"))\n",
        "\n",
        "    self.pred = []\n",
        "\n",
        "    self.model.compile(optimizer = \"rmsprop\", loss = \"sparse_categorical_crossentropy\", lr = model_specs.eta, metrics = [\"accuracy\"])\n",
        "\n",
        "  def predict(self, data, target):\n",
        "    return np.argmax(self.model.predict(data), axis = 1)\n",
        "    \n",
        "  def fit(self, data, target, valid_data):\n",
        "    return self.model.fit(data, target, validation_data = valid_data, epochs = model_specs.epoch, batch_size = model_specs.batch, verbose = 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGvjy-J4yfOR",
        "colab_type": "text"
      },
      "source": [
        "Neural network with 1 hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFoO-ZrCpNC_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81ee00ad-97e0-4b62-9c46-3f9be01f7eb7"
      },
      "source": [
        "\n",
        "nn_model = nn(reg= False, layer= 1)\n",
        "training_model = nn_model.fit(data = x_train, target = y_train, valid_data = (validation_x, validation_y))\n",
        "predicted_value = nn_model.predict(x_test, y_test)\n",
        "accuracy = accuracy_score(y_test, predicted_value)\n",
        "\n",
        "print(\"The accuracy of this model is \"+str(accuracy*100)+\"%\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of this model is 53.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUOKH70Ryj2I",
        "colab_type": "text"
      },
      "source": [
        "Neural network with 1 hidden layer and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzO7BU2Ix5_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f690f995-524d-4569-819c-36ee143c71e4"
      },
      "source": [
        "\n",
        "nn_model = nn(reg= True, layer= 1)\n",
        "training_model = nn_model.fit(data = x_train, target = y_train, valid_data = (validation_x, validation_y))\n",
        "predicted_value = nn_model.predict(x_test, y_test)\n",
        "accuracy = accuracy_score(y_test, predicted_value)\n",
        "\n",
        "print(\"The accuracy of this model is \"+str(accuracy*100)+\"%\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of this model is 37.25%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJkbPG5Yyrtx",
        "colab_type": "text"
      },
      "source": [
        "Neural network with 2 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtcVyamlyPAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2df9774d-c129-4359-8bb5-50d232cd32b1"
      },
      "source": [
        "\n",
        "nn_model = nn(reg= False, layer= 2)\n",
        "training_model = nn_model.fit(data = x_train, target = y_train, valid_data = (validation_x, validation_y))\n",
        "predicted_value = nn_model.predict(x_test, y_test)\n",
        "accuracy = accuracy_score(y_test, predicted_value)\n",
        "\n",
        "print(\"The accuracy of this model is \"+str(accuracy*100)+\"%\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of this model is 82.74000000000001%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6H_1XSyvOG",
        "colab_type": "text"
      },
      "source": [
        "Neural network with 2 hidden layers and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0wyJPiTyV0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f8f4a47-47b8-4eca-cccd-c009e75be64f"
      },
      "source": [
        "\n",
        "nn_model = nn(reg= True, layer= 2)\n",
        "training_model = nn_model.fit(data = x_train, target = y_train, valid_data = (validation_x, validation_y))\n",
        "predicted_value = nn_model.predict(x_test, y_test)\n",
        "accuracy = accuracy_score(y_test, predicted_value)\n",
        "\n",
        "print(\"The accuracy of this model is \"+str(accuracy*100)+\"%\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of this model is 9.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb2B5hxdy9tC",
        "colab_type": "text"
      },
      "source": [
        "Neural network with 3 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LSsZYJ_yaG5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1694e6a-91b8-4eb9-96db-29ba74a802f7"
      },
      "source": [
        "\n",
        "nn_model = nn(reg= False, layer= 3)\n",
        "training_model = nn_model.fit(data = x_train, target = y_train, valid_data = (validation_x, validation_y))\n",
        "predicted_value = nn_model.predict(x_test, y_test)\n",
        "accuracy = accuracy_score(y_test, predicted_value)\n",
        "\n",
        "print(\"The accuracy of this model is \"+str(accuracy*100))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of this model is 79.54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0kUJq1zzAUL",
        "colab_type": "text"
      },
      "source": [
        "Neural networks with 3 hidden layers and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1GbY6beybrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65059824-ca9e-4655-9195-6720e8616af0"
      },
      "source": [
        "\n",
        "nn_model = nn(reg= True, layer= 3)\n",
        "training_model = nn_model.fit(data = x_train, target = y_train, valid_data = (validation_x, validation_y))\n",
        "predicted_value = nn_model.predict(x_test, y_test)\n",
        "accuracy = accuracy_score(y_test, predicted_value)\n",
        "\n",
        "print(\"The accuracy of this model is \"+str(accuracy*100))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of this model is 9.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYt0C1xMF7ya",
        "colab_type": "text"
      },
      "source": [
        "### **2 c**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuXYRS3s1jkG",
        "colab_type": "text"
      },
      "source": [
        "Convoluted Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbuWkcAY1oi7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "baa022de-e6d2-4cbb-f5ab-17f36178035e"
      },
      "source": [
        "\n",
        "cnn_model = keras.models.Sequential()\n",
        "cnn_model.add(keras.layers.Conv2D(64, kernel_size=(3,3), activation= \"relu\", input_shape=(28,28,1)))\n",
        "cnn_model.add(keras.layers.Conv2D(32, kernel_size=(3,3), activation= \"relu\"))\n",
        "cnn_model.add(keras.layers.Flatten())\n",
        "cnn_model.add(keras.layers.Dense(10, activation= \"softmax\"))\n",
        "\n",
        "cnn_model.compile(optimizer= \"adam\", loss= \"categorical_crossentropy\", metrics= [\"accuracy\"])\n",
        "\n",
        "train_manipulate = ImageDataGenerator(featurewise_center= True, featurewise_std_normalization= True, rotation_range= 3, width_shift_range= 3, height_shift_range= 3, horizontal_flip= True)\n",
        "\n",
        "train_manipulate.fit(x_train.reshape(1000, 28, 28, 1))\n",
        "\n",
        "cnn_model.fit_generator(train_manipulate.flow(x_train.reshape(1000, 28, 28, 1), keras.utils.to_categorical(y_train), batch_size= model_specs.batch), steps_per_epoch= len(x_train.reshape(1000,28,28,1))/model_specs.batch, epochs= model_specs.epoch)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 1.4692 - accuracy: 0.5140\n",
            "Epoch 2/30\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.8434 - accuracy: 0.7360\n",
            "Epoch 3/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.6566 - accuracy: 0.7960\n",
            "Epoch 4/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.5316 - accuracy: 0.8340\n",
            "Epoch 5/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.5234 - accuracy: 0.8410\n",
            "Epoch 6/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.4907 - accuracy: 0.8550\n",
            "Epoch 7/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.4432 - accuracy: 0.8670\n",
            "Epoch 8/30\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.3906 - accuracy: 0.8750\n",
            "Epoch 9/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.4251 - accuracy: 0.8740\n",
            "Epoch 10/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.3590 - accuracy: 0.8820\n",
            "Epoch 11/30\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.3553 - accuracy: 0.8890\n",
            "Epoch 12/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.3174 - accuracy: 0.9030\n",
            "Epoch 13/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.3491 - accuracy: 0.9000\n",
            "Epoch 14/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.3145 - accuracy: 0.9060\n",
            "Epoch 15/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.2944 - accuracy: 0.9060\n",
            "Epoch 16/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.3055 - accuracy: 0.9020\n",
            "Epoch 17/30\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.2827 - accuracy: 0.9120\n",
            "Epoch 18/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.2790 - accuracy: 0.9100\n",
            "Epoch 19/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.2577 - accuracy: 0.9260\n",
            "Epoch 20/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.2301 - accuracy: 0.9290\n",
            "Epoch 21/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.2266 - accuracy: 0.9290\n",
            "Epoch 22/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.2279 - accuracy: 0.9220\n",
            "Epoch 23/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.2436 - accuracy: 0.9260\n",
            "Epoch 24/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.2305 - accuracy: 0.9260\n",
            "Epoch 25/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.1809 - accuracy: 0.9450\n",
            "Epoch 26/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.2261 - accuracy: 0.9340\n",
            "Epoch 27/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.2002 - accuracy: 0.9390\n",
            "Epoch 28/30\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 0.1970 - accuracy: 0.9420\n",
            "Epoch 29/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.1885 - accuracy: 0.9440\n",
            "Epoch 30/30\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.1745 - accuracy: 0.9480\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f767d4195c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKwjWeFM_6Nu",
        "colab_type": "text"
      },
      "source": [
        "Loss and accuracy of a Convoluted neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1sBGVGG7NkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6e009b4d-43ff-4283-eeae-941c88f35c47"
      },
      "source": [
        "\n",
        "cnn_pred = cnn_model.predict(x_test.reshape(10000, 28, 28, 1))\n",
        "cnn_loss, cnn_accuracy = cnn_model.evaluate(x_test.reshape(10000, 28, 28, 1), keras.utils.to_categorical(y_test))\n",
        "\n",
        "print(\"The loss incurred by CNN is \"+str(cnn_loss))\n",
        "print(\"The accuracy of CNN model is \"+str(round(cnn_accuracy*100, 2))+\"%\")\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 7s 653us/step\n",
            "The loss incurred by CNN is 47.87280726280213\n",
            "The accuracy of CNN model is 78.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djZ7wCaeR1ie",
        "colab_type": "text"
      },
      "source": [
        "1. Demonstrate that a NN maximizes the log likelihood of label is one that has softmax output nodes and maximizes the criterion function of the negative log probability of training dataset: $J_0(w)=-\\log(\\{(x_n,t_n):n=1,2...\\};w)=-\\log \\prod_n \\prod_{m=0}^9 p(t_n=m|x_n;w)$. Demonstrate that a NN maximizes the a posterior likelihood of observing the training data given a gaussian prior of the weight distribution $p(w;\\alpha)=N(0,\\alpha l)$ is one that maximizes the criterion function with L2 regularization $J(w)=J_0(w)-\\log p(w;\\alpha^{-1})$."
      ]
    }
  ]
}